(本文为转发)
源地址：http://www.xperseverance.net/blogs/2012/03/17/

若公式显示有问题请复制链接到新TAB重新打开

听说国外大牛都认为LDA只是很简单的模型，吾辈一听这话，只能加油了~

另外这个大牛写的LDA导读很不错：http://bbs.byr.cn/#!article/PR_AI/2530?p=1
一、预备知识：

       1. 概率密度和二项分布、多项分布，在这里

       2. 狄利克雷分布，在这里，主要内容摘自《Pattern Recognition and Machine Learning》第二章

       3. 概率图模型，在PRML第九章有很好的介绍
二、变量表示：

      1. word：word是最基本的离散概念，在自然语言处理的应用中，就是词。我觉得比较泛化的定义应该是观察数据的最基本的离散单元。word的表示可以是一个V维向量v，V是所有word的个数。这个向量v只有一个值等于1，其他等于0。呵呵，这种数学表示好浪费，我以前做过的项目里一般中文词在200-300w左右，每一个都表示成300w维向量的话就不用活了。哈哈，所以真正应用中word只要一个编号表示就成了。

     2. document：一个document就是多个word的合体。假设一篇文档有N个词，这些word是不计顺序的，也就是exchangeable的，LDA论文 3.1有说这个概念。论文中document的个数是M。

     3. topic：就是主题啦，比如“钱”的主题可能是“经济”，也可能是“犯罪”~ LDA中主题的表示是隐含的，即只预先确定主题的个数，而不知道具体的主题是什么。论文中表示主题个数的字母是k，表示主题的随机变量是z。

好了，总结一下所有的变量的意思，V是所有单词的个数（固定值），N是单篇文档词的个数（随机变量），M是总的文档的个数（固定值），k是主题的个数（需要预先根据先验知识指定，固定值）。

三、基础模型：

        先从两个基础模型说起：

       1. Unitgram model (LDA 4.1)

       一个文档的概率就是组成它的所有词的概率的乘积，这个一目了然，无需多说：

                            p(w)=∏n=1Np(wn)

       图模型：

      2. Mixture of unigrams (LDA 4.2)
        
       假如我们假设一篇文档是有一个主题的（有且仅有一个主题），可以引入主题变量z，那么就成了mixture of unigrams model。它的图模型如下图：

                        p(w)=∑zp(z)∏n=1Np(wn|z)
